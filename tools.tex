\chapter{Armory}
In the previous chapter I described the battlefield, the dangers you will face, but I also anticipated that there are new means for our protection. Following sections will cover some of the weapons available to our guardians of the cyberspace: not the \textit{Goldrake's Double Harken}, but they make a good impression anyway. 
\section{Intrusion Detection System}
You can imagine an IDS as the border police office and \cite{Khraisat2019} give a very deep analysis of it. An IDS, as its name suggests, is a device or software application that monitors a network for malicious activity or policy violations commonly called \textit{intrusions}. Intrusion can be defined as any kind of unauthorised activities that cause damage to an information system and the goal of an IDS is to identify different kinds of malicious network traffic and computer usage, which cannot be identified by a traditional firewall. IDS systems are broadly categorized into two groups: Signature-based Intrusion Detection System (SIDS) and Anomaly-based Intrusion Detection System (AIDS).
\subsection{Signature-based IDS}
Also knows as \textit{Knowledge-based Detection} or \textit{Misuse Detection}, SIDS  are based on pattern matching techniques to find a known attack: when an intrusion signature matches with the signature of a previous intrusion that already exists in the signature database, an alarm signal is triggered. For SIDS, host’s logs are inspected to find sequences of commands or actions which have previously been identified as malware. Building the signature database is the core step for SIDS and different techniques can be used as state machines, formal language string patterns or semantic conditions. This type of IDS are very effective against well-known intrusion techniques giving an excellent detection accuracy, but they have difficulty in detecting zero-day attacks (0 days are passed since the first occurrence of the vulnerability) for the reason that no matching signature exists in the database until the signature of the new attack is extracted and stored. Moreover SIDS are traditionally unable to identify attacks that span several network packets, while, as modern malware is more sophisticated, it may be necessary to extract signature information over multiple packets and recall the contents of earlier ones. A potential solution would be to use AIDS techniques.
\subsection{Anomaly-based IDS}
AIDS concept is fairly easy (although it is the more complex to implement of the two): a normal model of the behaviour of a computer system is created using machine learning, statistical-based or knowledge-based methods and any significant deviation between the observed behavior (what is happening now) and the model (what should happen) is regarded as an anomaly, which can be interpreted as an intrusion. AIDS development is divided in \textit{training phase}, when the normal traffic profile is analyzed to build the model and a \textit{testing phase}, when a new data set is used to test the efficiency of the IDS. AIDS overcomes the limitation of SIDS, making possible to recognize zero-day attacks due to the fact that doesn't rely on a signature database. Furthermore, AIDS has two main benefits: it has the capability to discover internal malicious activities (e.g. if an intruder starts making transactions in a stolen account that are unidentified in the typical user activity, it creates an alarm) and it is very difficult for a cyber criminal to recognize what is a normal user behavior without producing an alert as the system is constructed from customized profiles. However, AIDS can result in a high false positive rate because anomalies may just be new normal activities rather than genuine intrusions.
\subsubsection{Statistics-based}
A distribution model is built for normal behaviour profile, then low probability events are flagged as potential intrusions taking into account statistical metrics of packets: rather than inspecting data traffic, each packet is monitored. Statistical IDS normally use one of the following models.
\begin{itemize}
    \item \textbf{Univariate:} "Uni" means "one", data has only one variable. A single measure of behaviours in computer system is taken into account, looking for anomalies in each individual metric.
    \item \textbf{Multivariate:} Instead of analyzing data on a single measure, this model is based on relationships among two or more of them in order to understand the relationships between variables. The problem is that is very hard to estimate the distribution of high-dimensional data.
    \item \textbf{Time series model:} Multiple observations made over a certain time interval are called \textit{time series}. If the probability of a new observation of occurring at that time is too low, then it is labelled as abnormal.
\end{itemize}
\subsubsection{Knowledge-based}
The knowledge we are referring to reflects the legitimate traffic profile, any action which differs this standard profile is treated as an intrusion. The main difference with the two other methods is that in this case the model is created based on human knowledge. This techniques is able to reduce false-positive alarms since all the normal behaviours are modeled, but also is a very time consuming task due to the dynamically changing computing environment.
\begin{itemize}
    \item \textbf{Finite state machine (FSM):} Generally FSM is used to represent and control execution flow, but it can be applied in intrusion detection to produce a model represented in the form of states, transitions and activities. A state checks the logs of data and any variation reported triggers a specific transition: deviation from the FSM are reported as an attack.
    \item \textbf{Description Language:} Attacks are defined through a set of rules that specify their characteristics. These rules could be built using description languages such as \textit{N-grammars} and \textit{UML}.
    \item \textbf{Expert System:} Similar to description language model, but in this case the rules are defined by a knowledge engineer in collaboration with a domain expert.
\end{itemize}
\subsubsection{Machine learning-based}
Everyone talks about machine learning, but no one seems to fully understand its meaning. What interests us is to know that machine learning allow us to define patterns, predict behaviours or extract some knowledge in general from large quantities of data. Machine learning needs...to learn and two methods are available for this purpose: supervised and unsupervised learning. Supervised learning involves labeled training data (i.e. relevant features and classes are already defined) and the algorithm learns from it. Using the classified data samples this techniques trains a classifier able to define the most suitable class for new input data. Conversely unsupervised learning takes as input datasets without any class labels and the data is grouped automatically through the learning process. Unsupervised learning  has the advantages of requiring a minimal workload to manually classify the dataset and the greater freedom to identify and exploit undetected patterns that may not have been noticed by the "experts", but everything comes at a cost: greater amount of training data required to converge (slowly) to acceptable performance and increased computational and storage requirements.

\section{Security Operation Center}
If IDS is the border police, a SOC represents the ministry of defense. There is no commonly agreed-upon definition for a SOC, but \cite{Vielberth2020} did the job for me with their survey. The SOC is usually not seen as a separated entity, but rather a complex structure that aim to manage and enhance an organization's security posture combining people (who do the work), processes (how work is done) and technologies (what work is done with).
Its main feature is to detect, analyze and respond to cyber threats and incidents involving the three components mentioned above and the organization follows its directives for governance and compliance. Often SOC is confused with \textit{Computer Security Incident Response Team (CSIRT)}, even though the name should be clear enough: CSIRT, in fact, mainly focuses on the \textit{response} part, when an attack already happened, while SOC cover whole process, before-during-after attack. CSIRT is often part of the SOC or works in collaboration with it, the same way as \textit{Network Operations Center (NOC)} that focuses on incidents impacting the performance and availability of an organization’s network, the \textit{Security Information and Event Management (SIEM)} that is responsible for collecting security-relevant data in a centralized manner and is an integral part of many SOCs or \textit{Security Intelligence Center (SIC)} where several technologies like \textit{Information Security (IS)}, knowledge management or big data processing are combined to provide a more holistic and integrated view.\newline
SOC can be centralized, distributed or decentralized. In a centralized structure, all the data that need to be analyzed are sent from different locations or subsidiaries to one central SOC for processing. For an user there is no difference between this and a distributed structure since it appears as a single entity anyway, but in the second case all entities are able to retrieve, process, combine and provide security information and services to the others. The third approach is a combination of the previous ones: few SOCs with limited capabilities reporting to one or more central SOCs. Last option seems preferable to avoid a single point of failure, but everything depends on the final purpose and structure of the organization the SOC is serving.
SOC structure and operating model should be decided once and for all during implementation phase since changing them in a second moment will require a considerable amount of time and resources. Furthermore the choice of the operating model is not a trivial task and involves numerous factors: the company strategy to fit, the industry sector that influences the scope of the SOC, the size of the company (a small company can't sustain a SOC on its own) and the cost for implementing the SOC internally respect to the cost of relying on an external one, the time required, regulations that need to be followed due to the industry sector, the respect for privacy, availability requirements and management support, the integration level of the SOC with other IT departments, the level of confidence in the SOC (internal or external) regarding data loss and last, but not least, the expertise required for the staff.
\subsection{People}
Different roles and responsibilities exist within a SOC, the core ones are three tiers of analysts and their manager.
\begin{itemize}
    \item \textbf{Tier 1 (Triage Specialist):} Tier 1 acts in the same way of a hospital triage, analysts at this level need to confirm and evaluate the criticality of alerts and identify whether they are justified or false positives. If the alarm is justified, then it is prioritized and then solved according to its severity and, if necessary, escalated to tier 2. Triage analysts are in charge of managing and configuring the monitoring tools.
    \item \textbf{Tier 2 (Incident Responder):} More critical incidents escalate to tier 2 where they are analyzed in-depth using threat intelligence. Tier 2 analysts must understand the scope of the attack and be aware of the system involved in it to ensure that a valid containment and recovery strategy is designed and implemented. Bigger issues? Consult additional incident responders or escalate to tier 3.
    \item \textbf{Tier 3 (Threat Hunter):} If you reach tier 3 it means you are in big trouble! Threat hunters are the most experienced workforce in a SOC and also perform (or supervise) penetration tests and vulnerability assessments to recommend ways to optimize the monitoring tools. Any information from tier 1 and 2 is reviewed here.
    \item \textbf{SOC Manager:} SOC managers run the show. They, lo and behold, manage the SOC team, hiring, training and evaluating staff, creating processes, assessing reports and developing and implementing communication plans, not to mention that they oversee the financial aspects and report to the \textit{Chief Information Security Officer (CISO)} or a respective top-level management position.
\end{itemize}
In \cite{Vielberth2020} additional roles are identified and described further.\newline
This first component, as you may have understood, involve the human factor of the SOC and this comes with lot of issues. Being the last line of defense is very demanding and can be extremely stressful and companies should take action on this increasing automation, operational efficiency and human capital. Training, a mix of formal training, internal training, vendor-specific training, and on-the-job learning, is fundamental because it prevents employees from making mistakes and makes up for the lack of skilled staff, but there is very few literature about SOC-specific training methods and further research is necessary. Communication among colleagues is essential, especially in high-pressure environments like a SOC, but it is still rare also due to the absence of dedicated platforms.
\subsection{Process}
There are many ways to structure the processes involved within a SOC, but the most suitable one is through the \textit{Incident Response Lifecycle} (or similar frameworks) due to the fact that...SOC responds to incidents. A tailored Incident Response Lifecycle, suitable for SOC, is presented, again, by \cite{Vielberth2020}, the "SOC Bible".
\begin{itemize}
    \item \textbf{Preparation:} First step deals mostly with data collection since data is the only resource analysts can use to understand the incidents. Preparation consists in five steps (generally in this order): normalization to translate heterogeneous data formats into uniform representation for further processing, filtering to separate important information from useless ones, reduction of unimportant data fields to reduce amount of data, aggregation to combine similar or related data and prioritization to classify and prioritize data.
    \item \textbf{Detection and Analysis:} Now we have a huge amount of data and we have to make sense of it. Three steps to follow: detection, manual or automatic, to decide if the collected data indicates an incident or not, analysis to isolate simpler, synthesized and more accurate events and alert prioritization (triage) to allocate resources in the right order.
    \item \textbf{Containment, Eradication and Recovery:} This step is crucial to avoid that an incident overwhelms resources or increase damage. The standard procedure from \cite{Cichonski2012} involves decision-making in order to choose the right strategy to apply. When an incident is contained, administrators proceed to eliminate every trace of the incident left and restore the system to its original state. Anyway, as highlighted in the "bible", very few literature is available for SOCs.
\end{itemize}
Repeat with me: very little literature on the processes within a SOC is available and this lead many researchers to focus more on improving technologies with no clear reason. Noteworthy is the fact that the classic Incident Response Lifecycle includes a fourth step, post-incident activity, not even mentioned in SOC literature.
\subsection{Technology}
The term "technology" is not really suitable for this section, in my opinion, because it does not only includes sci-fi tools or cybernetic languages, but also interactions between humans. Anyway, the important aspect of the technology (I will call it this way for simplicity) is to be useful to the organization that implements it and to support the previous two components of the Incident Response Lifecycle. Taking the detection and analysis process as example, it can be performed either automatically using IDS or manually "using" a security novice that receives a phishing mail and then reports it to the security team that will take appropriate measures. The challenges about technology component are many as you might have expected. The increasing complexity of environment requires a wide variety of tools (with their cost of deployment and management) to deal with the data captured, heterogeneous as their sources. Having a simple and easy, but still precise and informative overview of the data is hard due to their huge amount and most of the times a trade-off between the requirements is needed. Lastly the automation problem: many tasks inside a SOC are carried out manually and this is caused by the fact that analysts' task are hard to automate.
\section{Computer Security Incident Response Team}
As anticipated earlier, CSIRT focuses on the reactive strategies against cyber incidents. CSIRT, aka \textit{Computer Emergency Response Team (CERT)}, can be see as the cybersecurity SWAT. In \cite{Mooi2015} a CSIRT is defined as \textit{"an organization or team that provides services and support to a defined constituency for preventing, handling, and responding to computer security incidents"}: an untrained eye might mistake this for a SOC. I have anticipated you some of the differences, but the example provided by \cite{Ruefle2014} may help you understand them better. In a SOC, initial alerts and reports coming from lower tiers are analyzed and, if an incident has occurred, higher tiers are involved in the incident management. The experts of tier 3 are most likely to be part of CSIRT that has the responsibility to perform response functions.\newline However, nothing prevents the CSIRT from assisting the whole process from start to finish. Depending on its mission, a CSIRT can perform reactive, proactive and security quality management services, directly or just providing input and information i.e. supervision. "The weakest link in the chain is still the humans" \cite{Ioannou2019}, which however is also the most important. As for SOC, CSIRT quality rely on its members' skills and they should be trained continuously due to the changes in attack trends, requiring time and financial resources. In addition, those who called humans the weakest link, highlighted that most of the obstacles that may rise inside a CSIRT are related to communication and coordination between the team members. Granny's advice: invest on development of cybersecurity culture to avoid lack of confidence and improve collaborative behaviour. Establishing a CSIRT is not a straightforward process and many aspect must be taken in consideration.
\subsection{Environment}
CSIRT environment specifies the sector in which the CSIRT will operate as well as geographic region of operations and (if any) also the organizational structure of hosting organization. \textit{Academic, CIP/CIIP (Critical Information an/or Infrastructure Protection), Government, Military, SME (Small and medium enterprises)} are the main sectors or \textit{business areas} that a CSIRT may serve, even more than one at time. Types, instead, differentiate the scope of CSIRT: \textit{National, Commercial, Internal, Vendor, Other.} The organizational environment defines also the model of CSIRT (their names aren't very fancy). Is CSIRT an independent entity with its own management and employees? Then we talk about \textit{independent} model which requires the most human resources as it must include administrative roles in addition to cyber experts. If the CSIRT is embedded within an organization, instead, the model is called \textit{embedded}. This model focuses on the re-use of existing human resources i.e. staff can be employed in the activities of CSIRT when needed, lowering the overheads. The \textit{campus} model is mostly adopted by academic and research CSIRTs. Academic organizations such as National Research Networks (NRENs) spread over regions or whole countries and comprehend multiple independent CSIRTs. A core CSIRT coordinates their tasks and is the single point of contact for the outside world. A more risky approach is represented by the \textit{voluntary} model in which a group of specialists join together to provide advice and support on a voluntary basis. The problem is that security-related work is subordinated to participants' other obligations and priorities. Different sectors and types have different scopes: a national CSIRT will focus on national level incidents while a company CSIRT (internal) will focus on protecting company's infrastructure and information (more individual-focused), a banking sector CSIRT will prioritize the protection of credit card information and online banking security while an academic sector CSIRT will be concerned about protecting students records and intellectual properties. The environment reveals the constituency of a CSIRT.
\subsection{Constituency}
CSIRT constituency defines who or what will be served by the team, be it a group of users, sites, networks or organizations. The clear and early definition of constituency is important because the people served need to know that they have a CSIRT (and this is not obvious) and in turn CSIRT and partners need to know who they are serving to appropriately direct reports and coordination. Constituency can be internal, external, centralized or distributed. Broad as a whole country or narrow like a single department of an organization. Typically the constituency is defined by IP address ranges, domain name, free text or autonomous system numbers. CSIRT constituency determines the authority and together with environment influences the funding model.
\subsection{Authority}
CSIRT authority describes the level of control that a CSIRT can exercise over the constituency. Four types of authority relationship are available: \textit{full, shared, indirect} and \textit{none}. A CSIRT with full authority is free to do whatever it needs to do, without the need to involve constituency. Shared authority allows the CSIRT to influence the decision-making process while a CSIRT with indirect authority can only exert pressure on constituency (e.g. ISP can disconnect services if actions are not taken). Needless to say that a CSIRT with none authority can only advise and hope it will be heard. Authority also affects the possibility to provide certain services: is impossible to perform intrusion detection or incident tracing without a proper level of authority.
\subsection{Funding and legal considerations}
CSIRT funding is an hard nut to crack too. The budget available influences the resources at disposal of CSIRT: without money you will end up on hunting hackers using \textit{AVAST antivirus}! The costs are determined by equipment and infrastructure, salaries and benefits for staff and operational and personal expenditures. About legal considerations, nothing too extravagant: environment also scopes the local laws and regulations (e.g. military CSIRT require confidentiality) that CSIRT must follow.

\section{Threat Modeling}
To prevent threats from overcoming the system, threat modeling methods can be used to think defensively. Creating an abstraction of the system, threat modeling methods highlight profiles of potential attackers (including their goals and attack vectors) and potential threat that may arise \cite{Shevchenko2018}. Threat modeling is associated with threat assessment.
\definition{Threat Assessment}{\begin{center}"Process of formally evaluating the degree of threat to an information system or enterprise and describing the nature of the threat" \cite{Paulsen2019}\medskip\end{center} The report card of threats. Once the possible threats have been listed, it is necessary to evaluate them to understand which one to eliminate first.}The best way to use threat modeling is doing it during early stages of development, allowing reduction of threats from the start. No threat modeling method is recommend over another and the decision on which to use, as always, depends on the needs of the user. Below, some example of most common methods.
\subsection{STRIDE}
STRIDE is the most mature threat modeling method and was adopted by Microsoft in 2002. STRIDE makes use of \textit{Data Flow Diagrams (DFDs)} to evaluate the system design: entities, events and boundaries of the system. The more accurate the DFD, the more successful STRIDE will be. Once we have a complete overview of the system we can start with step two. STRIDE is just an acronym that summarize a general set of known threats: \textbf{S}poofing identity, \textbf{T}ampering data, \textbf{R}epudiation, \textbf{I}nformation disclosure, \textbf{D}enial of service and \textbf{E}levation of privilege.

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{|c|c|c|X|} 
\hline
{\cellcolor{white}}\textbf{\textcolor{white}{}} &
{\cellcolor{dummy-cyan}}\textbf{\textcolor{white}{THREAT}} & 
{\cellcolor{dummy-cyan}}\textbf{\textcolor{white}{\begin{tabular}[c]{@{}c@{}}PROPERTY\\VIOLATED\end{tabular} }} & \multicolumn{1}{|c|}{{\cellcolor{dummy-cyan}}\textbf{\textcolor{white}{THREAT DEFINITION}}}\\ 

\hline
{\cellcolor{dummy-yellow}}S & Spoofing identify & Authentication                                         & Pretending to be something or someone other than yourself                              \\ 
\hline
{\cellcolor{dummy-yellow}}T & Tampering with data & Integrity                                              & Modifying something on disk, network, memory, or~elsewhere                             \\ 
\hline
{\cellcolor{dummy-yellow}}R & Repudiation & Non-repudiation                                        & Claiming that you didn’t do something or were not~responsible; can be honest~or false  \\ 
\hline
{\cellcolor{dummy-yellow}}I & Information disclosure & Confidentiality & Providing information to someone not authorized to~access it                           \\ 
\hline
{\cellcolor{dummy-yellow}}D & Denial of service & Availability                                           & Exhausting resources~needed to provide service                                         \\ 
\hline
{\cellcolor{dummy-yellow}}E & Elevation of privilege & Authorization                                          & Allowing someone to do~something they are not authorized to do                         \\
\hline
\end{tabularx}
\end{table}\noindent
While navigating the system's model, this mnemonic can be used for discovering threats. In addition, some sources offer checklists and tables that assist in describing threats, property violations, typical victims, and what an attacker does. All information gathered, then must be documented and prioritized. Being one of the oldest threat modeling methods, STRIDE had time to evolve and produce two variants: \textit{STRIDE-per-Element} and \textit{STRIDE-per-Interaction}. STRIDE-per-Element focuses on finding threats on each element of the system individually be they processes, data flow, external entities etc. For each element we have in the DFD, a "STRIDE Table" is built and we tick which threat category may arise on it. STRIDE-per-Interaction, instead, considers tuples of \textit{\{origin, destination, interaction\}} with the goal of enumerating threats against interactions between system's elements. STRIDE is easy to adopt, but very time consuming on complex systems (STRIDE-per-Interaction was developed to reduce the number of things that a modeler would have to consider, but that didn't work out as planned: same number of threats of STRIDE-per-Element, maybe a bit more easier to understand) and has a moderately high rate of false negatives in contrast with a moderately low rate of false positives.
\subsection{PASTA}
The name might be misleading if you love good food, but it's actually another acronym. The \textit{\textbf{P}rocess for \textbf{A}ttack \textbf{S}imulation and \textbf{T}hreat \textbf{A}nalysis (P.A.S.T.A)} is a risk-centric threat modeling framework developed in 2012. This method elevates the threat modeling process to a strategic level by involving key decision makers and requiring security input from operations, governance, architecture, and development. PASTA has seven stages, each supporting the next one: this model acts as a linear process that easily integrates within organization activities e.g. code review, third party library analysis, static analysis, and threat monitoring for application infrastructure. Widely regarded as a risk-centric framework, PASTA has also an attacker-centric perspective. PASTA approach always ties back to business context (unlike STRIDE which is much more static) and leverages existing processes from within the organization (easily scalable). 
\begin{figure}[H]
  \centering
  \includesvg[inkscapelatex=false,width=\textwidth]{pasta.svg}
  \caption{PASTA Stages}
\end{figure}
\subsection{CVSS}
The \textit{\textbf{C}ommon \textbf{V}ulnerability \textbf{S}coring \textbf{S}ystem} allows organizations to assess the characteristics of a vulnerability and its severity using a numerical score. Scores are calculated with a formula that depends on several metrics that approximate the ease and impact of an exploit. The score is expressed on a scale from 0 to 10, where 10 indicates the most serious vulnerability level. CVSS is well suited as a standard measurement system for industries, organizations, and governments that need accurate and consistent vulnerability severity scores. Two common uses of CVSS are calculating the severity of vulnerabilities discovered on one's systems and as a factor in prioritization of vulnerability remediation activities.
\begin{table}[H]
\centering
\begin{tabular}{|l|l|} 
\hline
\rowcolor{dummy-cyan} \textbf{\textcolor{white}{SEVERITY}} & \textbf{\textcolor{white}{SCORE RANGE}}  \\ 
\hline
None                                                 & \textcolor[rgb]{0.2,0.2,0.2}{0.0}             \\ 
\hline
Low                                                  & \textcolor[rgb]{0.2,0.2,0.2}{0.1-3.9}         \\ 
\hline
Medium                                               & \textcolor[rgb]{0.2,0.2,0.2}{4.0-6.9}         \\ 
\hline
High                                                 & 7.0-8.9                                       \\ 
\hline
Critical                                             & \textcolor[rgb]{0.2,0.2,0.2}{9.0-10.0}        \\
\hline
\end{tabular}
\end{table}\noindent
A CVSS score is composed of three sets of metrics: \textit{Base, Temporal, Environmental.} Base Factors represent characteristics of the vulnerability itself. These characteristics do not change over time and are not dependent on real world exploitability allowing the existence of public rankings of severity for known vulnerabilities. CVSS Temporal Metrics are related to a vulnerability that change over time. These metrics measure the current exploitability of the vulnerability, as well as the availability of remediating controls, such as a patch. Environmental Metrics are essentially modifiers to the Base metric group. These are designed to account for the aspects of an enterprise that might increase or decrease the net severity of a vulnerability. 
\section{Cyber Security Culture and Training}
The IT world has witnessed numerous revolutions, from the advent of the internet to modern smartphones and tablets, and each of them has brought with it some problems that have been solved over time. At its debut the internet was used only by those nerds, like Bill Gates, who stood all day in front of a screen as if their own life depended on it (joke, i love you Bill), also because not everyone had a computer at home. Digital divide is no longer our problem, even Bin Laden was able to send his videos from a cave in the middle of the desert and my grandmother can send me her blurry photos via Whatsapp. The problem now is: how are these new information tools used by citizens? Answer: bad! Virtually no one cares what happens while surfing the internet. \textit{"Do you accept cookies?"} Sure. \textit{"Do you want to subscribe to the newsletter?"} Of course. \textit{"You have won an IPhone, enter your details to receive it!"} Here I come! And then we are surprised when we discover that Google knows the life's history of all of us. Internet, intended as the set of IT tools, is like Las Vegas: what happens in Vegas, stays in Vegas. Forever! \cite{RONCHI2019}. A business manager is relatively concerned when a granny's personal data is stolen. However, the problem arises when an employee with her same IT security skills works inside the company. \cite{Reid2014} emphasize the importance of a \textit{Cyber Security Culture (CSC)} within an organization, particularly in an internet environment, making a distinction with the \textit{Information Security Culture (ISC)}. CSC recognizes the people as simultaneously assets, threats and vulnerabilities and this implies a less controlled social context with a wider range of skillsets, age and other variables. If a person does not follow rules for their own cyber security, it is very unlikely they will do so while at work. If an employee is used to clicking links without verifying the source when they consult their inbox at home, they will probably do so while they are at work. Having a solid CSC is equivalent to having the same mental elasticity as Jason Bourne: we will think twice before opening a suspicious link that warns us of a millionaire win and in case of threats we will know how to defend ourselves in time. At the end of the day we understood that it is important to develop a good CSC: in daily life, for our personal safety, but above all within the organization, to avoid endangering the entire company. Nice. How?
\subsection{The dojo}
In the middle of cyber warfare, we don't have time to wait by the river until the bodies of our enemies will float by, because, before that, we will have been fired and the company will face serious troubles. Our ace in the hole is \textit{training}. Like any self-respecting army, the guardians of the cyberspace also have their own shooting range, called \textit{cyber range}. Here they don't train with rifles and grenades, but simulating cyber attacks and defenses. Here our heroes sharpen their senses and perfect their strategies for when they will be on the real (cyber) battlefield. A cyber range is a realistic but safe environment in which exercises are carried out so that individuals, private and public organizations or government agencies can approach simulated critical situations. The simulations are based on "prepackaged" views of the possible designs of the cyber domain (\textit{modeling}). The model contains information on the network infrastructure, its vulnerabilities and the services provided and used, the security procedures applied and the threats that could affect it. The purpose of the simulation is to analyze the possible impact on the assets or the decision-making process, but above all to understand how the personnel would react in the created situation. \cite{Subasu2017} summarize simulations in three categories: \textit{real, virtual, constructive}. Real simulation is executed on real systems with real people in the form of security competitions in physical and isolated networks attended by specialists. Virtual simulation involves real people, but simulated systems focusing on training practical, control and cooperation capabilities. Constructive simulation, I don't need to tell you, is performed by using simulated systems operated by simulated entities, allowing personnel to insert entry data and exercise decision-making process. The latter are now widely used in traditional space (pew-pew, bang, boom) because they are less expensive, do not require special equipment (a real tank for example) and focus on planning, analysis and periodical evaluation (there is no need to deploy an entire platoon during each drill). On the contrary, as regards cyber security, organizations prefer real simulations which, albeit at a much higher cost, force attendants to consider time and space constraints and to collaborate, coordinate and compete with other teams. \cite{Debatty2019} recommend cyber range training also to improve \textit{Cyber Defense Situation Awareness (CDSA)}, key factor of decision-making process, subdivided in three levels: \textit{perception (Level 1)}, \textit{comprehension (Level 2)} and \textit{projection (Level 3)}. Perception is about how the expert/team perceive the real-time status, attributes and dynamics of relevant elements in cyberspace. Comprehension is about how the information are aggregated in order to understand their impacts on our goals and objectives. Projection is the ability to anticipate future events relaying on experience i.e. L1 and L2. Cyber range scenarios can be changed as needed and then adapted to improve a specific level of security awareness: to avoid information overload situations that may lead to miss critical information (L1), trainee, during a simulation, can be interrupted by both task related and non-task related distractions and submitted to peak workload (a kind of stress test).\newline
The name that has been given to this type of exercise is very trivial, but it reflects very well what we are talking about: \textit{Red team - Blue team exercise}. The trainees are divided into two teams, one that will try to compromise the simulated cyber domain (red) and one that will have the task of defending it (blue), both supervised by a team of experts who monitor their activities and gather information (white). As presented in \cite{Aoyama2015}, blue team's timeline is dependent to the red team as well as the resource allocation that changes dynamically to follow the progress of the attack. Both teams can receive tips or incentives from the white team so that the game will be carried as designed. The case taken as example shows how team play is fundamental (otherwise it would have been called "single red person vs single blue person exercise") and how a top-down decision making process, in which only the manager makes decisions, allows the other blue team members to focus on other tasks, but can create misunderstanding and friction between the management group and other members, leading to defeat. \cite{Ostby2019} suggest the combination with \textit{table-top exercises} especially when the goal is to test verbally procedures and plans and to differentiate the requirements and knowledge even for the instructors themselves based on who they will train. As proposed by \cite{Clark2015}, an \textit{Onion Approach} allows to alternate lessons of theory and practice with increasing difficulty. Activities focused on the exploitation of a single application, for example, are gradually increased to demonstrate the same type of exploitation on an entire subnet or from a remote machine. At each level of training is important to highlight the impact of exploitation on the target host, server, firewall and IDS to create a broader understanding of the security elements as a system rather just execute a single task. Operators who understand the principles behind security tools are able to adapt to different commercial tools. For this purpose, \cite{Karjalainen2020} introduce the concept of \textit{Cyber Arena} which, simplifying, consists of the union of multiple cyber ranges. Generally a cyber range focus on few specific functionalities, but in order to be able to teach the influences of real-world cyber environment in a realistic way, the training environment must implement most of these functionalities and provide a 360 degree view of the simulated world.
\section{Insurance}
When the going gets tough, sometimes the tough have to run for cover and find other solutions. Let's face it: a complete elimination of risks is not always possible and where the cyber warrior fails, the insurer takes over. Insurance is a key risk transfer mechanism that allows organizations to be financially protected in the event of loss or damage: financial risk is transferred from the company balance sheet to that of the insurer. In addition, the insurance encourages the insured to carry out their work better, improving the safety program, so that the cost is lowered: as in the case of the car insurance, by not causing accidents over time, the cost will drop \cite{Siegel2002}. If not, change the insurer! However, as \cite{Ogut2011} stated, sometimes organizations, especially when self-protection is not observable by insurer, rely too much on insurance (more than the socially optimal coverage), reducing investments on it. In any case it is important to make sure that the insurance product we rely on is specifically suitable against cyber risks and not that it only has the word "cyber" on it. A properly crafted insurance program must provide \textit{Web Content Liability, Internet Professional Liability} and \textit{Network Security Coverage}. Web Content Liability covers claims arising out of the content of your web site such as libel, slander, copyright and trademark infringement. Internet Professional Liability covers claims arising out of the performing of professional services e.g. when a computer software organization develops a new product that damages a client’s computer system. Network Security Coverage comes in two parts: \textit{Third-Party Coverage} and \textit{First-Party Coverage}. Third-Party Coverage provides liability coverage arising from a failure of the insured’s security to prevent unauthorized use or access of its network. This would cover claims arising from transmission of virus, theft of a customer’s information or DOS liability. First-Party Coverage provides reimbursement for loss arising out of information assets issues, whether or not criminal, such as alteration, copying, theft and destruction.

